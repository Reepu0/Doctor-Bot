{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8BZDGBO8siP",
        "outputId": "fa9aa1fb-d943-44d4-aa2f-7b61653ae2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ The full system has been successfully downloaded.\n",
            "\n",
            "üöÄ Interactive Diagnostic System\n",
            "Write down the symptoms one by one and then 'Diagnose' to diagnose, or 'Exit' to end the session.\n",
            "\n",
            "üîç Enter a symptom or 'diagnose' or 'Exit': Exit\n",
            "üõë The system has been terminated.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import joblib\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    data['symptoms'] = data['symptoms'].astype(str).str.lower()\n",
        "    data['symptoms'] = data['symptoms'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['Disease label'] = label_encoder.fit_transform(data['disease'])\n",
        "    return data, label_encoder\n",
        "\n",
        "def prepare_datasets(data, test_size=0.2):\n",
        "    X = data['symptoms']\n",
        "    y = data['Disease label']\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "class BERTDiagnoser:\n",
        "    def __init__(self, model_name='roberta-base', max_len=128, batch_size=16, epochs=10):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def encode_texts(self, texts):\n",
        "        return self.tokenizer(list(texts), padding=True, truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "    def create_data_loader(self, texts, labels):\n",
        "        encodings = self.encode_texts(texts)\n",
        "        dataset = BERTDataset(encodings, labels)\n",
        "        return DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def train(self, train_loader, num_classes):\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=num_classes).to(self.device)\n",
        "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "        model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'\\nüß† Epoch {epoch + 1}/{self.epochs}')\n",
        "            for batch in tqdm(train_loader):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model\n",
        "\n",
        "class TraditionalModels:\n",
        "    def __init__(self):\n",
        "        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def vectorize_texts(self, texts):\n",
        "        return self.sbert_model.encode(list(texts), show_progress_bar=False)\n",
        "\n",
        "    def vectorize_text(self, text):\n",
        "        return self.sbert_model.encode([text])[0]\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        X_train_vec = self.vectorize_texts(X_train)\n",
        "\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "            'SVM': SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42),\n",
        "            'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
        "            'Naive Bayes': GaussianNB()\n",
        "        }\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nüöÄ Training {name}...\")\n",
        "            model.fit(X_train_vec, y_train)\n",
        "            preds = model.predict(X_train_vec)\n",
        "            print(f\"{name} F1-Score:\\n{classification_report(y_train, preds)}\")\n",
        "\n",
        "        return models, self.sbert_model\n",
        "\n",
        "    def predict_proba(self, models, sbert_model, text):\n",
        "        vec = sbert_model.encode([text])[0].reshape(1, -1)\n",
        "        results = {}\n",
        "        for name, model in models.items():\n",
        "            results[name] = model.predict_proba(vec)[0]\n",
        "        return results\n",
        "\n",
        "class DiseaseDiagnosisSystem:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.label_encoder = None\n",
        "        self.sbert_model = None\n",
        "        self.bert_model = None\n",
        "        self.bert_tokenizer = None\n",
        "        self.model_dir = './saved_models'\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "        self.model_weights = {}\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        self.data, self.label_encoder = load_and_preprocess_data(file_path)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = prepare_datasets(self.data)\n",
        "\n",
        "    def train_models(self):\n",
        "        trad = TraditionalModels()\n",
        "        self.models, self.sbert_model = trad.train(self.X_train, self.y_train)\n",
        "        X_train_vec = trad.vectorize_texts(self.X_train)\n",
        "\n",
        "        self.model_weights = {}\n",
        "        for name, model in self.models.items():\n",
        "            preds = model.predict(X_train_vec)\n",
        "            f1 = f1_score(self.y_train, preds, average='weighted')\n",
        "            self.model_weights[name] = f1\n",
        "\n",
        "        bert_model = BERTDiagnoser()\n",
        "        train_loader = bert_model.create_data_loader(self.X_train.values, self.y_train.values)\n",
        "        self.bert_model = bert_model.train(train_loader, len(self.label_encoder.classes_))\n",
        "        self.bert_tokenizer = bert_model.tokenizer\n",
        "\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for text in self.X_train:\n",
        "                inputs = self.bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(self.bert_model.device)\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                pred = torch.argmax(outputs.logits, dim=1).cpu().item()\n",
        "                preds.append(pred)\n",
        "        f1 = f1_score(self.y_train, preds, average='weighted')\n",
        "        self.model_weights['BERT'] = f1\n",
        "\n",
        "      # ‚úÖ Maintain order after training\n",
        "        self.save_system()\n",
        "\n",
        "    def save_system(self):\n",
        "        try:\n",
        "            joblib.dump((self.models, self.sbert_model), os.path.join(self.model_dir, 'traditional_models.pkl'))\n",
        "            joblib.dump(self.label_encoder, os.path.join(self.model_dir, 'label_encoder.pkl'))\n",
        "            self.bert_model.save_pretrained(os.path.join(self.model_dir, 'bert_model'))\n",
        "            self.bert_tokenizer.save_pretrained(os.path.join(self.model_dir, 'bert_model'))\n",
        "            joblib.dump(self.model_weights, os.path.join(self.model_dir, 'model_weights.pkl'))\n",
        "            print(\"‚úÖ The entire system has been saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå System save failed:\", e)\n",
        "\n",
        "    def load_system(self):\n",
        "        try:\n",
        "            self.models, self.sbert_model = joblib.load(os.path.join(self.model_dir, 'traditional_models.pkl'))\n",
        "            self.label_encoder = joblib.load(os.path.join(self.model_dir, 'label_encoder.pkl'))\n",
        "            self.bert_model = AutoModelForSequenceClassification.from_pretrained(os.path.join(self.model_dir, 'bert_model'))\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(os.path.join(self.model_dir, 'bert_model'))\n",
        "            self.bert_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "            self.model_weights = joblib.load(os.path.join(self.model_dir, 'model_weights.pkl'))\n",
        "            print(\"‚úÖ The full system has been successfully downloaded.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå System loading failed:\", e)\n",
        "            return False\n",
        "\n",
        "    def load_models(self):\n",
        "        return self.load_system()\n",
        "\n",
        "    def predict(self, symptoms):\n",
        "        trad = TraditionalModels()\n",
        "        trad_preds = trad.predict_proba(self.models, self.sbert_model, symptoms)\n",
        "        inputs = self.bert_tokenizer(symptoms, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        inputs = {k: v.to(self.bert_model.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert_model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "        trad_preds['BERT'] = probs\n",
        "\n",
        "        weighted_probs = sum(self.model_weights[name] * trad_preds[name] for name in trad_preds)\n",
        "        avg_probs = weighted_probs / sum(self.model_weights.values())\n",
        "\n",
        "        pred_idx = np.argmax(avg_probs)\n",
        "        predicted_disease = self.label_encoder.inverse_transform([pred_idx])[0]\n",
        "        confidence = avg_probs[pred_idx]\n",
        "        max_contributor = max(trad_preds.items(), key=lambda x: x[1][pred_idx])[0]\n",
        "        top_diseases = [(self.label_encoder.inverse_transform([i])[0], avg_probs[i]) for i in np.argsort(avg_probs)[::-1]]\n",
        "\n",
        "        return {\n",
        "            'disease': predicted_disease,\n",
        "            'confidence': confidence,\n",
        "            'main_contributor': max_contributor,\n",
        "            'details': trad_preds,\n",
        "            'top_7': top_diseases[:7]\n",
        "        }\n",
        "\n",
        "    def evaluate_on_external_data(self, file_path):\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['symptoms'] = df['symptoms'].astype(str).str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "            true_labels = self.label_encoder.transform(df['disease'])\n",
        "\n",
        "            pred_labels = []\n",
        "            for i in range(len(df)):\n",
        "                result = self.predict(df.iloc[i]['symptoms'])\n",
        "                pred = self.label_encoder.transform([result['disease']])[0]\n",
        "                pred_labels.append(pred)\n",
        "\n",
        "            print(\"\\nüìä Performance Evaluation:\")\n",
        "            print(classification_report(true_labels, pred_labels, target_names=self.label_encoder.classes_))\n",
        "            print(\"\\nüß© Confusion Matrix:\")\n",
        "            print(confusion_matrix(true_labels, pred_labels))\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Failure to evaluate external data:\", e)\n",
        "\n",
        "    def interactive_diagnosis(self):\n",
        "        print(\"\\nüöÄ Interactive Diagnostic System\")\n",
        "        print(\"Write down the symptoms one by one and then 'Diagnose' to diagnose, or 'Exit' to end the session.\\n\")\n",
        "        symptoms_list = []\n",
        "\n",
        "        while True:\n",
        "            symptom = input(\"üîç Enter a symptom or 'diagnose' or 'Exit': \").strip()\n",
        "            if symptom.lower() in ['exit', 'quit','bay']:\n",
        "                print(\"üõë The system has been terminated.\")\n",
        "                break\n",
        "            elif symptom.lower() in ['diagnose','ok']:\n",
        "                if not symptoms_list:\n",
        "                    print(\"‚ö†Ô∏è No symptoms entered.\")\n",
        "                    continue\n",
        "                full_symptoms = \" \".join(symptoms_list)\n",
        "                result = self.predict(full_symptoms)\n",
        "                print(f\"\\n‚úÖ Diagnosis: {result['disease']} ({result['confidence']*100:.2f}%)\")\n",
        "                print(f\"üèÜ Best Contributing Model: {result['main_contributor']}\")\n",
        "                print(\"\\nüìä Top 7 diseases:\")\n",
        "                for i, (disease, prob) in enumerate(result['top_7'], 1):\n",
        "                    print(f\"{i}. {disease}: {prob*100:.2f}%\")\n",
        "                symptoms_list = []\n",
        "            elif symptom.lower() == 'retraining':\n",
        "                self.load_data('/content/last_skin_disease_symptoms_dataset.csv')\n",
        "                self.train_models()\n",
        "                print(\"‚úÖ Models have been retrained.\")\n",
        "            elif symptom.lower().startswith('evaluation'):\n",
        "                parts = symptom.split()\n",
        "                if len(parts) == 2:\n",
        "                    self.evaluate_on_external_data(parts[1])\n",
        "                else:\n",
        "                    print(\"‚ùå Type: evaluate <file_path>\")\n",
        "            else:\n",
        "                symptoms_list.append(symptom)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    system = DiseaseDiagnosisSystem()\n",
        "    if not system.load_models():\n",
        "        system.load_data('/content/last_skin_disease_symptoms_dataset.csv')\n",
        "        system.train_models()\n",
        "    system.interactive_diagnosis()\n"
      ]
    }
  ]
}